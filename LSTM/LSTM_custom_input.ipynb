{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T20:10:33.659939Z",
     "iopub.status.busy": "2021-03-09T20:10:33.659274Z",
     "iopub.status.idle": "2021-03-09T20:10:33.666407Z",
     "shell.execute_reply": "2021-03-09T20:10:33.665502Z"
    },
    "papermill": {
     "duration": 0.027971,
     "end_time": "2021-03-09T20:10:33.666686",
     "exception": false,
     "start_time": "2021-03-09T20:10:33.638715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T20:10:38.434450Z",
     "iopub.status.busy": "2021-03-09T20:10:38.433686Z",
     "iopub.status.idle": "2021-03-09T20:10:45.537423Z",
     "shell.execute_reply": "2021-03-09T20:10:45.536891Z"
    },
    "papermill": {
     "duration": 7.128449,
     "end_time": "2021-03-09T20:10:45.537568",
     "exception": false,
     "start_time": "2021-03-09T20:10:38.409119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Import Libraries \n",
    "import re\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sklearn.svm import LinearSVC\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Input\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T20:10:45.580523Z",
     "iopub.status.busy": "2021-03-09T20:10:45.579726Z",
     "iopub.status.idle": "2021-03-09T20:10:47.253553Z",
     "shell.execute_reply": "2021-03-09T20:10:47.252983Z"
    },
    "papermill": {
     "duration": 1.699932,
     "end_time": "2021-03-09T20:10:47.253709",
     "exception": false,
     "start_time": "2021-03-09T20:10:45.553777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Reading csv data files using pandas dataframe \n",
    "\n",
    "train = pd.read_csv(\"/Users/ved/Desktop/Sem 7/DL Project/Dataset/train.csv\", encoding = \"ISO-8859-1\")\n",
    "test = pd.read_csv(\"/Users/ved/Desktop/Sem 7/DL Project/Dataset/test.csv\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T20:10:47.429226Z",
     "iopub.status.busy": "2021-03-09T20:10:47.428593Z",
     "iopub.status.idle": "2021-03-09T20:10:47.443431Z",
     "shell.execute_reply": "2021-03-09T20:10:47.443929Z"
    },
    "papermill": {
     "duration": 0.043497,
     "end_time": "2021-03-09T20:10:47.444135",
     "exception": false,
     "start_time": "2021-03-09T20:10:47.400638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Before cleaning the dataset I would like to perform EDA(Exploratory data analysis) by performing data visualization to understand\n",
    "### the distribution of different classes. I will be performing EDA on training dataset\n",
    "\n",
    "categorywise_data = train.drop(['id', 'comment_text'], axis=1)     ### Removed unnecessary columns - id and comment_text\n",
    "counts_category = []                                               ### A list that contains tuple which consists of class label and number of comments for that particular class \n",
    "categories = list(categorywise_data.columns.values)\n",
    "for i in categories:\n",
    "    counts_category.append((i, categorywise_data[i].sum()))\n",
    "    \n",
    "dataframe = pd.DataFrame(counts_category, columns=['Labels', 'number_of_comments'])   ### Dataframe made up of category and total number of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T20:10:47.949043Z",
     "iopub.status.busy": "2021-03-09T20:10:47.948111Z",
     "iopub.status.idle": "2021-03-09T20:10:48.186249Z",
     "shell.execute_reply": "2021-03-09T20:10:48.185502Z"
    },
    "papermill": {
     "duration": 0.268003,
     "end_time": "2021-03-09T20:10:48.186415",
     "exception": false,
     "start_time": "2021-03-09T20:10:47.918412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Visualization 2\n",
    "\n",
    "### Bar graph of Total No. of labels in a sentence against Total no. of sentences\n",
    "### This visualization is helpful in identifying whether a sentence belongs to only one category or many categories\n",
    "\n",
    "dataframe = pd.DataFrame(pd.DataFrame(train[train.columns[2:]].sum(axis=1)).reset_index()[0].value_counts())\n",
    "dataframe[\"Total no. of sentences\"]=dataframe[0]\n",
    "dataframe[\"Total No. of labels in a sentence\"]=dataframe.index\n",
    "\n",
    "### From the below graph we can see that 1,43,346 out of 1,59,571 sentences does not have any labels(class 0).\n",
    "### we can observe that a single sentence can have multiple labels. It can be a toxic sentence or it can be a toxic as well as obscene senetence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T20:10:48.975288Z",
     "iopub.status.busy": "2021-03-09T20:10:48.974560Z",
     "iopub.status.idle": "2021-03-09T20:10:49.496302Z",
     "shell.execute_reply": "2021-03-09T20:10:49.497051Z"
    },
    "papermill": {
     "duration": 0.554164,
     "end_time": "2021-03-09T20:10:49.497297",
     "exception": false,
     "start_time": "2021-03-09T20:10:48.943133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Visualization 4\n",
    "\n",
    "### Correlation between different variables\n",
    "### Correlation helps us finding relationship/dependency between different variables. \n",
    "\n",
    "target_data = train.drop(['id', 'comment_text'], axis=1)\n",
    "\n",
    "\n",
    "### Correlation coefficient ranges from -1 to 1. Values always range between -1 imply strong negative relationship\n",
    "### between variables and +1 imply a strong positive relationship between variables. \n",
    "### Values at or close to zero imply weak or no linear relationship. \n",
    "### From the correlation matrix(graph), it can be concluded that some labels are highely correlated. Those varibles are mentioned below \n",
    "### (Correlation coefficient for insult-obscene is 0.74, Correlation coefficient for toxic-obscene is 0.68 and Correlation coefficient for toxic-insult is 0.65)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T20:11:25.100366Z",
     "iopub.status.busy": "2021-03-09T20:11:25.099657Z",
     "iopub.status.idle": "2021-03-09T20:11:25.109572Z",
     "shell.execute_reply": "2021-03-09T20:11:25.110036Z"
    },
    "papermill": {
     "duration": 0.037581,
     "end_time": "2021-03-09T20:11:25.110254",
     "exception": false,
     "start_time": "2021-03-09T20:11:25.072673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Splitting up the labels and data\n",
    "### Training dataset is splitted into 2 parts. 1st part includes the training data(train_data) and 2nd part includes labels(train_label) \n",
    "### associated with the training data\n",
    "### Test dataset is having only 1 part i.e. test data which is used to predict the labels. \n",
    "\n",
    "train_data = train[\"comment_text\"]\n",
    "test_data = test[\"comment_text\"]\n",
    "train_label=train[['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1m/v10vjxqd235fn30s91s6nnx40000gn/T/ipykernel_19822/50109776.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_combined[\"comment\"][i] = ast.literal_eval(df_combined[\"comment\"][i])\n",
      "/var/folders/1m/v10vjxqd235fn30s91s6nnx40000gn/T/ipykernel_19822/50109776.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_combined_test[\"comment\"][i] = ast.literal_eval(df_combined_test[\"comment\"][i])\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "df_combined = pd.read_csv('/Users/ved/Desktop/Sem 7/DL Project/Tokenized_Data/df_combined.csv')\n",
    "df_combined_test = pd.read_csv('/Users/ved/Desktop/Sem 7/DL Project/Tokenized_Data/df_combined_test.csv')\n",
    "\n",
    "for i in range(len(df_combined['comment'])):\n",
    "    df_combined[\"comment\"][i] = ast.literal_eval(df_combined[\"comment\"][i])\n",
    "\n",
    "for i in range(len(df_combined_test['comment'])):\n",
    "    df_combined_test[\"comment\"][i] = ast.literal_eval(df_combined_test[\"comment\"][i])\n",
    "\n",
    "X_train1 = df_combined['comment']\n",
    "X_test1 = df_combined_test['comment']\n",
    "\n",
    "max_features = 30000\n",
    "maxlen = 150\n",
    "embed_size = 300\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train1) + list(X_test1))\n",
    "X_train = tokenizer.texts_to_sequences(X_train1)\n",
    "X_test = tokenizer.texts_to_sequences(X_test1)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data (159571, 150)\n",
      "Shape of testing data (63978, 150)\n"
     ]
    }
   ],
   "source": [
    "train_padded = x_train\n",
    "test_padded = x_test\n",
    "print(\"Shape of training data\",train_padded.shape)\n",
    "print(\"Shape of testing data\",test_padded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T20:11:25.161621Z",
     "iopub.status.busy": "2021-03-09T20:11:25.160935Z",
     "iopub.status.idle": "2021-03-09T20:11:41.858171Z",
     "shell.execute_reply": "2021-03-09T20:11:41.857515Z"
    },
    "papermill": {
     "duration": 16.72358,
     "end_time": "2021-03-09T20:11:41.858337",
     "exception": false,
     "start_time": "2021-03-09T20:11:25.134757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Creating corpus of words and coverting it into integer and then susbstituing it in sentences - prepare tokenizer\n",
    "tokenizer = Tokenizer(num_words = 40000) #40000 words are used here\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "\n",
    "#convert each text into array of integers with help of tokenizer.\n",
    "train_final = tokenizer.texts_to_sequences(train_data)\n",
    "test_final = tokenizer.texts_to_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T20:11:41.920697Z",
     "iopub.status.busy": "2021-03-09T20:11:41.915628Z",
     "iopub.status.idle": "2021-03-09T20:11:45.474218Z",
     "shell.execute_reply": "2021-03-09T20:11:45.473352Z"
    },
    "papermill": {
     "duration": 3.591319,
     "end_time": "2021-03-09T20:11:45.474432",
     "exception": false,
     "start_time": "2021-03-09T20:11:41.883113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data (159571, 150)\n",
      "Shape of testing data (153164, 150)\n"
     ]
    }
   ],
   "source": [
    "### Padding - Every sentence is unequal in length. We need to have all the sentence equal in lengths\n",
    "### If length of sentence is less than 150 then padding will increase sentence length to 150 by adding zeros, if its greater\n",
    "### than 150 then it will reduce the length of sentence to 150 by trimming the words\n",
    "\n",
    "train_padded =pad_sequences(train_final, maxlen=150)\n",
    "test_padded =pad_sequences(test_final, maxlen=150)\n",
    "print(\"Shape of training data\",train_padded.shape)\n",
    "print(\"Shape of testing data\",test_padded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train1) + list(X_test1))\n",
    "def custom(input):\n",
    "    input = tokenizer.texts_to_sequences(input)\n",
    "    input = sequence.pad_sequences(input, maxlen=maxlen)\n",
    "    pred = model.predict(input)\n",
    "    toxicity_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "    j=0\n",
    "    for i in pred[0]:\n",
    "        if i > 0.5:\n",
    "            pred[0][j]=1\n",
    "        else:\n",
    "            pred[0][j]=0\n",
    "        j = j+1\n",
    "    for i, class_name in enumerate(toxicity_classes):\n",
    "        print(f\"{class_name}: {pred[0][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "toxic: 1.0\n",
      "severe_toxic: 0.0\n",
      "obscene: 1.0\n",
      "threat: 0.0\n",
      "insult: 1.0\n",
      "identity_hate: 0.0\n"
     ]
    }
   ],
   "source": [
    "input= [\"shit\"]\n",
    "custom(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1045.304424,
   "end_time": "2021-03-09T20:27:53.353386",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-09T20:10:28.048962",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
