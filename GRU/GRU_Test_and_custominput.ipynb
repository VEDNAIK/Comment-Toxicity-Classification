{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('GRU.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.read_csv('/Users/ved/Desktop/Sem 7/DL Project/Tokenized_Data/df_combined.csv')\n",
    "df_combined_test = pd.read_csv('/Users/ved/Desktop/Sem 7/DL Project/Tokenized_Data/df_combined_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"/Users/ved/Desktop/Sem 7/DL Project/Dataset/test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_combined['comment'])):\n",
    "    df_combined[\"comment\"][i] = ast.literal_eval(df_combined[\"comment\"][i])\n",
    "for i in range(len(df_combined_test['comment'])):\n",
    "    df_combined_test[\"comment\"][i] = ast.literal_eval(df_combined_test[\"comment\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = df_combined['comment']\n",
    "X_test1 = df_combined_test['comment']\n",
    "max_features = 30000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 18s 287ms/step\n"
     ]
    }
   ],
   "source": [
    "X_test = tokenizer.texts_to_sequences(X_test1)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "y_pred = model.predict(x_test, batch_size=1024)\n",
    "y_pred = (y_pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = df_combined_test\n",
    "y_actual.drop(columns=\"comment\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8662196380005627\n",
      "Precision: 0.5359717248755987\n",
      "Recall: 0.7949372327217548\n",
      "F1-score: 0.6402599927779785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming y_actual contains the true labels and y_predicted contains the predicted labels\n",
    "# You can replace these with your actual and predicted labels # Replace with your true labels\n",
    "y_predicted = y_pred # Replace with your predicted labels\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_actual, y_predicted)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(y_actual, y_predicted,average='micro')\n",
    "recall = recall_score(y_actual, y_predicted,average='micro')\n",
    "f1 = f1_score(y_actual, y_predicted,average='micro')\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "input= [\"shit\"]\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train1) + list(X_test1))\n",
    "def custom(input):\n",
    "    input = tokenizer.texts_to_sequences(input)\n",
    "    input = sequence.pad_sequences(input, maxlen=maxlen)\n",
    "    pred = model.predict(input)\n",
    "    toxicity_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "    j=0\n",
    "    for i in pred[0]:\n",
    "        if i > 0.5:\n",
    "            pred[0][j]=1\n",
    "        else:\n",
    "            pred[0][j]=0\n",
    "        j = j+1\n",
    "    for i, class_name in enumerate(toxicity_classes):\n",
    "        print(f\"{class_name}: {pred[0][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "toxic: 1.0\n",
      "severe_toxic: 0.0\n",
      "obscene: 1.0\n",
      "threat: 0.0\n",
      "insult: 0.0\n",
      "identity_hate: 0.0\n"
     ]
    }
   ],
   "source": [
    "custom(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
